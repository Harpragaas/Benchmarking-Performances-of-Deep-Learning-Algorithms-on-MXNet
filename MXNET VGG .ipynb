{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import gluon, nd, autograd\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data.vision import transforms, CIFAR10\n",
    "import time, os, shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "batch_size = 16\n",
    "trainset = CIFAR10(root = './data', train = True).transform_first(transform)\n",
    "trainloader = gluon.data.DataLoader(trainset, batch_size = 16, shuffle = True, num_workers = batch_size)\n",
    "testset = CIFAR10(root = './data', train = False).transform_first(transform)\n",
    "testloader = gluon.data.DataLoader(testset, batch_size = 16, shuffle = True, num_workers = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.min(): -1.0\n",
      "features.max(): 1.0\n",
      "labels: \n",
      "[3 3 8 2 0 0 2 0 4 8 1 0 7 8 1 0]\n",
      "<NDArray 16 @cpu_shared(0)>\n",
      "labels.min(): 0\n",
      "labels.max(): 8\n"
     ]
    }
   ],
   "source": [
    "for features, labels in trainloader:\n",
    "    print('features.min():', features.min().asscalar())\n",
    "    print('features.max():', features.max().asscalar())\n",
    "    \n",
    "    print('labels:', labels)\n",
    "    print('labels.min():', labels.min().asscalar())\n",
    "    print('labels.max():', labels.max().asscalar())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Block):\n",
    "    def __init__(self):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = self._make_layers()\n",
    "        self.classifier = nn.Dense(10)\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.reshape((out.shape[0], -1))\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "    def _make_layers(self):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        vgg = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', \n",
    "               512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M']\n",
    "        for x in vgg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2D(pool_size = 2, strides = 2)]\n",
    "            else:\n",
    "                layers += [nn.Conv2D(in_channels, kernel_size = 3, padding=1),\n",
    "                           nn.BatchNorm(),\n",
    "                           nn.Activation('relu')]\n",
    "                in_channels = x\n",
    "        layers += [nn.AvgPool2D(pool_size = 1, strides = 1)]\n",
    "        model = nn.Sequential()\n",
    "        model.add(*layers)\n",
    "        return model\n",
    "ctx = mx.cpu()\n",
    "model = VGG()\n",
    "model.initialize(ctx = ctx)\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(model.collect_params(), 'adam', {'learning_rate': 0.001})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "start = time.time()\n",
    "for epoch in range(10):\n",
    "    batch_start = time.time()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader):\n",
    "        inputs, labels = data\n",
    "        with autograd.record():\n",
    "            outputs = model(inputs.as_in_context(ctx))\n",
    "            loss = objective(outputs, labels.as_in_context(ctx)).mean()\n",
    "        loss.backward()\n",
    "        trainer.step(16)\n",
    "        running_loss += loss\n",
    "        losses.append(loss.asscalar())\n",
    "    \n",
    "    print(f'Epoch: {(epoch)} | Loss: {running_loss.asscalar() / (i + 1):.5f} | Images/Sec: {(batch_size * (i + 1)) / (time.time() - batch_start):.5f}')\n",
    "        \n",
    "        \n",
    "    running_loss = 0.0\n",
    "print('\\nFinished Training')\n",
    "print(f'Total time taken: {(time.time() - start)/60.0:.5f} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
